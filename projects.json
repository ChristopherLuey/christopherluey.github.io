{
  "projects": [
    {
      "title": "Lagrangian Dynamics Simulation",
      "description": "A simulation of a gymnastics high bar dismount using Lagrangian dynamics and numerical methods.<br><br><span class=\"class-with\">Class with Prof. Todd Murphey (Northwestern University)</span>",
      "media": ["images/1/0726.mov"], 
      "link": "project.html?title=Lagrangian%20Dynamics%20Simulation",
      "details": {
        "pageTitle": "Lagrangian Dynamics Simulation",
        "projectDescription": "Lagrangian dynamics simulation<br><br><span class=\"class-with\">Class with Prof. Todd Murphey (Northwestern University)</span>",
        "detailedDescription": "<p><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">I generated a 2D dynamic simulation of a gymnastics high bar flip using Python. The dynamics are solved using governing physics equations and numerical simulation.</span></p><h3><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:13pt;\"><strong>Methods Used:</strong></span></h3><ul><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Symbolic Computation (SymPy)</strong>: For deriving the Euler-Lagrange dynamic equations.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Fourth Order Numerical Solvers</strong>: For accurately modeling projectile motion during the dismount.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Reference Frame Transformations</strong>: Using SE(2) rotational matrices to account for the orientation and position of the rigid bodies.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Variational Calculus</strong>: To derive the governing equations from the Lagrangian, considering kinetic and potential energy.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Rigid Body Contact</strong>: Modeled as plastic impact for when the body hits the ground.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Constraint Forces</strong>: Applied while the gymnast is swinging on the bar.</span></li></ul><h3><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:13pt;\"><strong>Simulation Phases:</strong></span></h3><p><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Phase One (Double Pendulum)</strong>:</span></p><ul><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Begins with bodies in a vertically upright position with initial angular velocities.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Arms (Body B) rotate around the high bar (2.8m high) until a specified release angle is reached.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Uses the Euler-Lagrange equation to model dynamics and constraints.</span></li></ul><p><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Phase Two (Projectile Motion)</strong>:</span></p><ul><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Upon reaching the release angle, the constraint on the arms is removed.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Both bodies follow a projectile trajectory.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">Simulation continues until a plastic impact is detected when the torso (Body D) hits the ground.</span></li><li><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">The impact is successful when both corners of the torso contact the ground simultaneously.</span></li></ul><p><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">The system is modeled with two rigid bodies representing the human arms and torso, each with definite volume and mass.</span></p><p><span style=\"background-color:transparent;font-family:Calibri,sans-serif;font-size:11pt;\">The code is here: https://github.com/ChristopherLuey/ME314/blob/main/Chris_Final_Project.ipynb</span></p>",
        "media": ["images/1/IMG_0410_Original.jpeg", "images/1/0726.mov", "images/1/1.mov", "images/1/IMG_0415_Original.jpeg", "images/1/IMG_0413_Original.jpeg"]
      }
    },
    {
      "title": "Pick and Place Robotic Manipulation",
      "description": "End-effector trajectory planning, odometry, inverse kinematics.<br><br><span class=\"class-with\">Class with Prof. Kevin Lynch (Northwestern University)</span>",
      "media": ["images/2/0725.mov"],
      "link": "project.html?title=Pick%20and%20Place%20Robotic%20Manipulation",
      "details": {
        "pageTitle": "Pick and Place Robotic Manipulation",
        "projectDescription": "End-effector trajectory planning, odometry, inverse kinematics.<br><br><span class=\"class-with\">Class with Prof. Kevin Lynch (Northwestern University)</span>",
        "detailedDescription": "<p><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\">To complete the pick and place task, I created a custom controller from scratch for a robot equipped with a 5R joint arm and 4 mecanum wheels using Python. This involved several advanced techniques and meticulous tuning to ensure precise and reliable operation.</span></p><p><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:13pt;\"><strong>Methods Used:</strong></span></p><ul><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Odometry</strong>: Assuming no slip, I calculated the position and orientation of the robot base using kinematic equations. This method involved continuously tracking the wheel rotations and applying the appropriate transformations to determine the robot's pose in the workspace. Accurate odometry is critical for ensuring that the robot can navigate and position itself correctly for picking and placing objects.</span></li><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Feedback Control</strong>: I implemented and tuned a PID controller to follow the end-effector trajectory accurately. The PID controller adjusts the manipulator's movements based on:</span>
        <ul><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\">The current error (Proportional term): This provides an immediate response to the current discrepancy between the desired and actual position.</span></li><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\">The accumulated error over time (Integral term): This helps to eliminate steady-state errors that accumulate over time.</span></li><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\">The rate of error change (Derivative term): This predicts future errors based on the current rate of change, improving the stability and responsiveness of the controller.</span></li></ul></li><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Singularity Detection and Avoidance</strong>: Using Jacobian matrices, I adjusted my controller to avoid singularities, which are positions where the robot loses certain degrees of freedom and control becomes unreliable. By calculating the Jacobian matrix and its determinant, I could identify near-singular configurations and modify the control inputs to steer clear of these problematic areas, ensuring smooth and consistent operation.</span></li><li><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\"><strong>Trajectory Planning</strong>: Using SE(3) transformation matrices, I generated an end-effector trajectory that the robot followed to pick and place objects. SE(3) transformations account for both rotational and translational movements in 3D space, allowing for precise and smooth path generation. The trajectory planning involved defining waypoints and using interpolation techniques to create a continuous path for the end-effector to follow, ensuring that the robot moves efficiently and accurately between pick and place locations.</span></li></ul><p><span style=\"color:#212121;font-family:Calibri,sans-serif;font-size:11pt;\">My project code is here: https://github.com/ChristopherLuey/ME449</span></p>",
        "media": ["images/2/0725.mov", "images/2/Milestone2.mov", "images/2/overshoot_simulation.mp4", "images/2/part3a.mov"]
      }
    },
    {
      "title": "6D Object Pose Estimation",
      "description": "Using RGB video to estimate YCB object poses for tele-operated robot.<br><br><span class=\"class-with\">Research with Prof. Ed Colgate (Northwestern University)</span>",
      "media": ["images/3/3.mov"],
      "link": "project.html?title=6D%20Object%20Pose%20Estimation",
      "details": {
        "pageTitle": "Research: 6D Object Pose Estimation",
        "projectDescription": "Using RGB video to estimate YCB object poses for tele-operated robot.<br><br><span class=\"class-with\">Research with Prof. Ed Colgate (Northwestern University)</span>",
        "detailedDescription": "<p>This project focuses on 6D object pose estimation, which involves determining the SE(3) transformation that maps an object's body frame to the camera frame using RGB and RGBD images. This technique is crucial for applications in robotics, augmented reality, and autonomous driving.</p><p>The process of 6D pose estimation includes:</p><ul><li><strong>Bounding Box Object Detection:</strong> Identifying objects in the scene using bounding boxes.</li><li><strong>Deep Neural Network Processing:</strong> Using neural networks to process the bounding box information and estimate the 6D pose.</li></ul><p>The project explored several methodologies:</p><ul><li><strong>Template-Based Methods:</strong> Utilized predefined models to match against observed data. Effective for objects with unique shapes but struggled with occlusions and required a large number of templates.</li><li><strong>Point-Based Methods:</strong> Focused on matching specific geometric points. Suitable for rigid objects but sensitive to noise and required distinct points for matching.</li><li><strong>Local Descriptor-Based Methods:</strong> Described local areas around key points using descriptors. Robust to lighting changes and occlusions but computationally intensive.</li><li><strong>Feature-Based Methods:</strong> Used distinctive features like edges and corners for matching. Effective in various conditions but struggled with low-texture objects and occlusions.</li></ul><p>The project also involved advanced deep learning techniques:</p><ul><li><strong>PoseCNN:</strong> Classified 3D rotation and translation separately using semantic labels and CAD models.</li><li><strong>DeepIM:</strong> Predicted optical flow to aid pose estimation and used auxiliary branches for optical flow and mask prediction.</li><li><strong>DenseFusion:</strong> Combined RGB and depth data at the pixel level for accurate pose prediction.</li></ul><p>Challenges addressed included dealing with occlusions, varying lighting conditions, and the need for extensive training data. The project also explored integration of depth sensing technologies to enhance pose estimation accuracy.</p><p>Skills learned and applied:</p><ul><li>Advanced computer vision techniques.</li><li>Deep learning algorithms for object detection and pose estimation.</li><li>Handling RGB and RGBD data for real-time applications.</li><li>Development and implementation of neural network models.</li></ul><p>This research significantly advanced the understanding and application of 6D object pose estimation, providing robust solutions for real-world applications in robotics and beyond.</p>",
        "media": ["images/3/3.mov"]
      }
    },
    {
      "title": "Online Robotic Gait Control using Generative AI",
      "description": "Using an autoencoder to learn and generate control sequences for robot walking.<br><br><span class=\"class-with\">Research with Prof. Todd Murphey (Northwestern University)</span>",
      "media": ["images/4/4.mov"],
      "link": "project.html?title=Online%20Robotic%20Gait%20Control%20using%20Generative%20AI",
      "details": {
        "pageTitle": "Research: Online Robotic Gait Control using Generative AI",
        "projectDescription": "Using an autoencoder to learn and generate control sequences for robot walking.<br><br><span class=\"class-with\">Research with Prof. Todd Murphey (Northwestern University)</span>",
        "detailedDescription": "<p>This project introduces a novel generative autoencoder framework for online robotic gait learning. The method utilizes a conditional variational autoencoder (CVAE) to generate structured control signals in unknown environments with unfamiliar robot dynamics. The approach incorporates a closed-loop feedback pipeline that learns from active robot ambulation and online control generation.</p><p>The pipeline consists of three stages:</p><ul><li><strong>Sampling:</strong> The encoder, decoder, and reward functions are sampled to characterize minimal loss regions, identifying promising gait descriptors for further investigation.</li><li><strong>Execution:</strong> The robot executes a selected minimal loss control signal in its environment, and its reward function is characterized.</li><li><strong>Training:</strong> The reward function and reconstructed outputs are backpropagated for weight training.</li></ul><p>The project was implemented using PyTorch for neural network development and CUDA for GPU acceleration, enabling efficient training and inference. The Mujoco Ant simulation was used for initial testing, showing that good gaits are well-represented in the latent space with minimal reconstruction loss.</p><p>The CVAE training was constrained to gaits achieving specific rewards (0.35 < x < 0.4 [m] and -0.1 < y < 0.1 [m]), and the model learned general latent features for these gaits. The framework also integrated:</p><ul><li><strong>Latent Space Analysis:</strong> Analyzing latent representations of good gait descriptors for skill-based learning.</li><li><strong>Policy Optimization:</strong> Using an optimizer and policy π loss function to refine control signals.</li><li><strong>Noise and Variational Uncertainty:</strong> Incorporating noise and uncertainty in the latent space for robust learning.</li></ul><p>Future work includes integrating this online learning approach with a physical robot and refining the latent space reward functions for more targeted learning.</p><p>Skills learned and applied in this project include:</p><ul><li>Advanced deep learning techniques using CVAE.</li><li>Implementation of neural networks using PyTorch and CUDA.</li><li>Real-time control signal generation and feedback loop design.</li><li>Simulation and validation using the Mujoco Ant environment.</li></ul><p>This research significantly advances the field of online robotic gait learning, providing robust solutions for adaptive control in dynamic environments.</p>",
        "media": ["images/4/4.mov", "images/4/4.jpeg"]
      }
    },
    {
      "title": "A Digital Twin Framework for Predictive Maintenance using Temporal Fusion Transformers",
      "description": "Using a temporal fusion transformer<br><br><span class=\"class-with\">Research with Prof. Wei Chen (Northwestern University)</span>",
      "media": ["images/5/5.jpeg"],
      "link": "project.html?title=A%20Digital%20Twin%20Framework%20for%20Predictive%20Maintenance%20using%20Temporal%20Fusion%20Transformers",
      "details": {
        "pageTitle": "Research: A Digital Twin Framework for Predictive Maintenance using Temporal Fusion Transformers",
        "projectDescription": "Using a temporal fusion transformer<br><br><span class=\"class-with\">Research with Prof. Wei Chen (Northwestern University)</span>",
        "detailedDescription": "<p>This project introduces a novel digital twin framework for predictive maintenance using Probabilistic Temporal Fusion Transformers (TFTs). The primary objective is to optimize tire resource allocation by predicting tire wear and extending the lifespan of vehicle tires, thereby reducing environmental impact and mechanical waste.</p><p>The framework integrates several advanced techniques and methods:</p><ul><li><strong>Temporal Fusion Transformer Architecture:</strong> Utilized for processing time-series data to predict future states of tires. Key components include static covariate encoders, LSTM encoder-decoder, and masked multi-head attention mechanisms.</li><li><strong>Digital Twin Method:</strong> A hybrid approach combining physical models (Finite Element Method - FEM) with machine learning models to monitor and predict the mechanical lifetime of systems.</li><li><strong>Real-Time Adaptation and Decision Making:</strong> The system employs a Tire State Decision Algorithm (TSDA) to dynamically update tire health predictions and make maintenance decisions based on real-time data.</li><li><strong>Reinforcement Learning for Resource Allocation:</strong> A reinforcement learning (RL) framework is developed to optimize tire swaps and rotations, ensuring even wear and maximizing tire life. The RL agent learns strategies from data to make optimal decisions under varying conditions.</li></ul><p>The implementation involved the following steps:</p><ol><li><strong>Data Collection:</strong> Time-series data from vehicle sensors, including tire pressure, casing temperature, load, speed, road curvature, and mileage.</li><li><strong>Data Processing:</strong> Gaussian kernel smoothing and adaptive down-sampling to handle large datasets efficiently while preserving critical features.</li><li><strong>Model Training:</strong> The TFT model was trained using PyTorch with a focus on minimizing Mean Absolute Percentage Error (MAPE). The TFT model outperformed traditional models such as LSTM, RNN, and GRU.</li><li><strong>Integration with Physical Model:</strong> The TFT model predictions are combined with FEM-based tire models to enhance prediction accuracy.</li><li><strong>Validation and Results:</strong> The model was validated using real-world data from Michelin's fleet, demonstrating accurate predictions of tire degradation and remaining mileage.</li></ol><p>The project showcased significant advancements:</p><ul><li>Improved lifetime prediction accuracy with TFT, achieving a MAPE of 29.5 compared to higher errors in other models.</li><li>Real-time decision-making capabilities for maintenance actions, reducing unnecessary tire replacements and extending tire usage.</li><li>Substantial environmental benefits by reducing CO2 emissions, energy consumption, and raw material usage.</li></ul><p>This research represents a significant step towards sustainable and efficient predictive maintenance in the automotive industry, with potential applications in various other mechanical systems.</p>",
        "media": ["images/5/5.jpeg"]
      }
    },
    {
      "title": "Image Analysis Particle Tracking",
      "description": "NDA.<br><br><span class=\"class-with\">Research with Prof. Juan Santiago (Stanford University)</span>",
      "media": ["images/6/6.JPG"],
      "link": "project.html?title=Image%20Analysis%20Particle%20Tracking",
      "details": {
        "pageTitle": "Research: Novel Computer Vision Particle Tracking",
        "projectDescription": "NDA.<br><br><span class=\"class-with\">Research with Prof. Juan Santiago (Stanford University)</span>",
        "detailedDescription": "<p>Computer vision</p>",
        "media": ["images/6/6.JPG"]
      }
    }
  ]
}
